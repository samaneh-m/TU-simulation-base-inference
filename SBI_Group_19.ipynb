{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samaneh-m/TU-simulation-base-inference/blob/main/SBI_Group_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference of protein secondary structure motifs\n",
        "###Group 19. Sama and Ana Alonso asd"
      ],
      "metadata": {
        "id": "n--sOFxzCZO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proteins are long chains of amino acids that fold into specific shapes. One key level of organization is the secondary structure, where each amino acid is part of three local folding patterns (alphahelix, beta-sheet or random coil [1]), which then further fold into three-dimensional structures, defining the function of the protein. In this project, we focus specifically on predicting alphahelix patterns using a two-state Hidden Markov Model (HMM) [2]. The two states are \"alphahelix\" and \"other\" (encompassing beta-sheets and coils). We assume fixed emission and transition probabilities derived from empirical data [3].\n",
        "\n",
        "Wedefinethefollowing generative model for simulating amino acid sequences: The sequence always starts in the \"other\" state. We output an amino acid based on the following tables of emission probabilities:\n",
        "\n",
        "We are also using these transition probabilities: If the current state is \"alpha-helix\", the next state is \"alpha-helix\" with probability p = 90% and \"other\" with probability 1 p = 10%. If the the current state is \"other\", the next state is \"alpha-helix\" with probability p = 005 and \"other\" with probability 1 p = 95%. Using this simulator, we can simulate amino acid chains of arbitrary length. Additionally, with the Viterbi algorithm [4], it is also possible to infer state probabilities for a given amino acid sequence, e.g. using the hmmlearn Python package [5]. Given pairs of amino acid sequences and state probabilities as training data, the goal is to use BayesFlow to train a neural posterior density estimator. Then, compare the posterior state probability estimates for a new protein sequence to the known ground truth, for example, the annotate secondary structure of human insulin [6]."
      ],
      "metadata": {
        "id": "scoNYgAACPBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###References\n",
        "* 1. https://old-ib.bioninja.com.au/higher-level/topic-7-nucleic-acids/73-translation/protein-structure.html\n",
        "* 2. https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf\n",
        "* 3. https://www.kaggle.com/datasets/alfrandom/protein-secondary-structure\n",
        "* 4. https://web.stanford.edu/~jurafsky/slp3/A.pdf\n",
        "* 5. https://pypi.org/project/hmmlearn/\n",
        "* 6. https://www.rcsb.org/3d-sequence/1A7F"
      ],
      "metadata": {
        "id": "9juQzNTVCkoP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JylhIdN_CF1G"
      },
      "outputs": [],
      "source": [
        "# prompt: create simple code for this section\n",
        "\n",
        "# This notebook explores the application of Hidden Markov Models (HMMs)\n",
        "# to the task of protein secondary structure prediction, specifically focusing\n",
        "# on identifying alpha-helical regions.\n",
        "\n",
        "# We will simulate amino acid sequences using a two-state HMM\n",
        "# (\"alpha-helix\" and \"other\") with predefined emission and transition probabilities.\n",
        "# Subsequently, we aim to use BayesFlow to train a neural posterior density estimator\n",
        "# to infer the hidden states (secondary structure) given an amino acid sequence.\n",
        "\n",
        "# The project will involve:\n",
        "# 1. Defining the HMM structure (states, emission probabilities, transition probabilities).\n",
        "# 2. Simulating amino acid sequences based on the defined HMM.\n",
        "# 3. Using an HMM inference algorithm (like Viterbi) for ground truth state sequences.\n",
        "# 4. Utilizing BayesFlow for neural posterior estimation.\n",
        "# 5. Evaluating the performance of the trained estimator on a new protein sequence.\n",
        "\n",
        "# Libraries that might be needed:\n",
        "# !pip install hmmlearn # For HMM operations\n",
        "# !pip install bayesflow # For neural posterior estimation\n",
        "# !pip install numpy # For numerical operations\n",
        "# !pip install pandas # For data handling (if needed)\n",
        "# !pip install matplotlib # For plotting (if needed)\n",
        "\n",
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# Define the states\n",
        "states = [\"alpha-helix\", \"other\"]\n",
        "n_states = len(states)\n",
        "\n",
        "# Define the possible amino acids\n",
        "# This is a simplified representation, a full set of 20 amino acids would be used in a real scenario\n",
        "amino_acids = list(\"ARNDCEQGHI\") # Example subset of amino acids\n",
        "n_amino_acids = len(amino_acids)\n",
        "\n",
        "# Assume emission probabilities (replace with actual values from source [3])\n",
        "# This is a placeholder example:\n",
        "# Rows represent states (alpha-helix, other)\n",
        "# Columns represent amino acids (A, R, N, D, C, E, Q, G, H, I)\n",
        "emission_probs = np.array([\n",
        "    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], # Example for alpha-helix\n",
        "    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]  # Example for other\n",
        "])\n",
        "\n",
        "# Assume transition probabilities (as described in the text)\n",
        "# Rows represent current state (alpha-helix, other)\n",
        "# Columns represent next state (alpha-helix, other)\n",
        "transition_probs = np.array([\n",
        "    [0.90, 0.10], # From alpha-helix\n",
        "    [0.05, 0.95]  # From other\n",
        "])\n",
        "\n",
        "# Initial state probability (starts in \"other\")\n",
        "start_prob = np.array([0.0, 1.0])\n",
        "\n",
        "# Create the HMM model\n",
        "model = hmm.MultinomialHMM(n_components=n_states, random_state=42)\n",
        "model.startprob_ = start_prob\n",
        "model.transmat_ = transition_probs\n",
        "model.emissionprob_ = emission_probs\n",
        "\n",
        "print(\"HMM model defined.\")\n",
        "print(\"States:\", states)\n",
        "print(\"Amino Acids (example):\", amino_acids)\n",
        "print(\"Transition Probabilities:\\n\", model.transmat_)\n",
        "print(\"Emission Probabilities:\\n\", model.emissionprob_)\n",
        "print(\"Initial State Probability:\", model.startprob_)\n",
        "\n",
        "# Future steps would involve:\n",
        "# 1. Implementing sequence simulation using model.sample().\n",
        "# 2. Implementing inference (e.g., Viterbi) using model.decode().\n",
        "# 3. Integrating with BayesFlow for neural posterior estimation.\n",
        "# 4. Downloading and processing real protein data (e.g., from RCSB PDB).\n"
      ]
    }
  ]
}