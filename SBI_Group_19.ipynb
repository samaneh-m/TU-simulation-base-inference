{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samaneh-m/TU-simulation-base-inference/blob/main/SBI_Group_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference of protein secondary structure motifs\n",
        "###Group 19. Sama and Ana Alonso asd"
      ],
      "metadata": {
        "id": "n--sOFxzCZO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proteins are long chains of amino acids that fold into specific shapes. One key level of organization is the secondary structure, where each amino acid is part of three local folding patterns (alphahelix, beta-sheet or random coil [1]), which then further fold into three-dimensional structures, defining the function of the protein. In this project, we focus specifically on predicting alphahelix patterns using a two-state Hidden Markov Model (HMM) [2]. The two states are \"alphahelix\" and \"other\" (encompassing beta-sheets and coils). We assume fixed emission and transition probabilities derived from empirical data [3].\n",
        "\n",
        "Wedefinethefollowing generative model for simulating amino acid sequences: The sequence always starts in the \"other\" state. We output an amino acid based on the following tables of emission probabilities:\n",
        "\n",
        "We are also using these transition probabilities: If the current state is \"alpha-helix\", the next state is \"alpha-helix\" with probability p = 90% and \"other\" with probability 1 p = 10%. If the the current state is \"other\", the next state is \"alpha-helix\" with probability p = 005 and \"other\" with probability 1 p = 95%. Using this simulator, we can simulate amino acid chains of arbitrary length. Additionally, with the Viterbi algorithm [4], it is also possible to infer state probabilities for a given amino acid sequence, e.g. using the hmmlearn Python package [5]. Given pairs of amino acid sequences and state probabilities as training data, the goal is to use BayesFlow to train a neural posterior density estimator. Then, compare the posterior state probability estimates for a new protein sequence to the known ground truth, for example, the annotate secondary structure of human insulin [6]."
      ],
      "metadata": {
        "id": "scoNYgAACPBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###References\n",
        "* 1. https://old-ib.bioninja.com.au/higher-level/topic-7-nucleic-acids/73-translation/protein-structure.html\n",
        "* 2. https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf\n",
        "* 3. https://www.kaggle.com/datasets/alfrandom/protein-secondary-structure\n",
        "* 4. https://web.stanford.edu/~jurafsky/slp3/A.pdf\n",
        "* 5. https://pypi.org/project/hmmlearn/\n",
        "* 6. https://www.rcsb.org/3d-sequence/1A7F"
      ],
      "metadata": {
        "id": "9juQzNTVCkoP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JylhIdN_CF1G"
      },
      "outputs": [],
      "source": []
    }
  ]
}